#***************************************************************
# High level stuff
[DEFAULT]
save_dir = saves/default
#data_dir = data
data_dir = /home/betul/turkunlp/Turku-neural-parser-pipeline
lc = tr
treebank = Turkish
lang = Turkish 

[Configurable]
#train_files = %(data_dir)s/CoNLL17/UD_%(treebank)s/%(lc)s-ud-train.conllu
train_files = %(data_dir)s/train/%(lc)s-ud-train.conllu
#train_files = %(data_dir)s/new_treebank/tr_imst-ud-train.conllu

#parse_files = %(data_dir)s/CoNLL17/UD_%(treebank)s/%(lc)s-ud-dev.conllu
parse_files = %(data_dir)s/dev/%(lc)s-ud-dev.conllu
#parse_files = %(data_dir)s/new_treebank/tr_imst-ud-dev.conllu

verbose = True
name = None

#***************************************************************
# Vocab data structures
[Base Vocab]
special_tokens=<PAD>:<ROOT>
# TODO take special_tokens out of here and put them in the classes
cased = None
embed_size = 300

[Pretrained Vocab]
special_tokens=<PAD>:<ROOT>:<DROP>:<UNK>
skip_header = True
name = pretrained
#filename = %(data_dir)s/embeddings/%(lang)s/%(lc)s.vectors.xz
filename = %(data_dir)s/models_tr_imst/Data/%(lc)s.vectors
#filename = /home/betul/lstm-char-parser/lstm-parser/build/embedding-files/wiki.tr.vec
cased = False
max_rank = 0

[Token Vocab]
name = tokens
embed_keep_prob = .67
min_occur_count = 2
max_rank = 100000

[Index Vocab]
special_tokens=<PAD>:<ROOT>

[Dep Vocab]
name = deps

[Head Vocab]
name = heads

[Word Vocab]
special_tokens=<PAD>:<ROOT>:<DROP>:<UNK>
name = words
filename = %(save_dir)s/%(name)s.txt
cased = False

[Lemma Vocab]
name = lemmas
filename = %(save_dir)s/%(name)s.txt

[Tag Vocab]
special_tokens=PAD:ROOT:DROP:UNK
name = tags
filename = %(save_dir)s/%(name)s.txt
cased = True

[X Tag Vocab]
name = xtags
filename = %(save_dir)s/%(name)s.txt

[Rel Vocab]
special_tokens=pad:root:drop:unk
name = rels
filename = %(save_dir)s/%(name)s.txt
cased = True

[Subtoken Vocab]
max_rank = 0
# TODO Setting this to more than 1 triggers a bug
n_buckets = 2
embed_model = CNNEmbed
embed_keep_prob = 1

[Char Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>:<START>:<STOP>
name = chars
filename = %(save_dir)s/%(name)s.txt
embed_model = RNNEmbed

[Ngram Vocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>
name = ngrams
filename = %(save_dir)s/%(name)s.txt
embed_model = MLPEmbed

[Ngram Multivocab]
special_tokens = <PAD>:<ROOT>:<DROP>:<UNK>:<META_UNK>
name = multi-ngram
max_n = 5
embed_model = MLPEmbed

[Bytepair Vocab]
name = bytepairs
filename = %(save_dir)s/%(name)s.txt
n_bytepairs = 500
embed_model = MLPEmbed

[Multivocab]
embed_keep_prob = .67

#***************************************************************
# Neural models
[NN]
recur_cell = LSTMCell
n_layers = 3
mlp_func = leaky_relu
conv_func = leaky_relu
# TODO make sure you add this to Base Cell
recur_size = 200
window_size = 5
conv_size = 200
mlp_size = 200
rnn_func = birnn
conv_keep_prob = .67
mlp_keep_prob = .67
recur_keep_prob = .67
ff_keep_prob = .67

[Base Cell]
forget_bias = 0
recur_func = tanh
recur_size = 300

[RNN Cell]
recur_func = leaky_relu
recur_size = 400

[Base Embed]

[MLP Embed]

[RNN Embed]
rnn_func = rnn

[CNN Embed]

[Base Tagger]
input_vocabs = words
output_vocabs = tags

[Base X Tagger]
input_vocabs = words
output_vocabs = tags:xtags

[Tagger]
name = tagger
n_layers = 2
recur_keep_prob = .5

[X Tagger]
name = xtagger
n_layers = 2
recur_keep_prob = .5

[Base Parser]
# TODO take off xtags later
input_vocabs = words:tags:xtags
output_vocabs = rels:heads

[Parser]
name = parser
arc_mlp_size = 400
rel_mlp_size = 100

[Xbar Parser]
name = xbar_parser
p_mlp_size = 400
arc_mlp_size = 400
rel_mlp_size = 100

[Bin Parser]
name = bin_parser
p_mlp_size = 400
arc_mlp_size = 400
rel_mlp_size = 100

[Fish Parser]
name = fish_parser
lambda_mlp_size = 400
arc_mlp_size = 400
rel_mlp_size = 100

[Gama Parser]
name = fish_parser
p_mlp_size = 400
arc_mlp_size = 400
rel_mlp_size = 100

[Joint Parser]
tag_mlp_size = 500
arc_mlp_size = 500
rel_mlp_size = 100

#***************************************************************
# Sequence data structures
[Multibucket]
n_buckets = 2
name = multibucket

[Bucket]
name = None

[Dataset]
#TODO make sure you can get rid of data_files

[Trainset]
name = trainset
data_files = train_files
n_buckets = 10
batch_by = tokens
batch_size = 5000

[Parseset]
name = parseset
data_files = parse_files
n_buckets = 5
batch_by = tokens
batch_size = 50000


#***************************************************************
# Training 
[Network]
name = network
subtoken_vocab = CharVocab
nlp_model = Parser
min_train_iters = 1000
max_train_iters = 30000
validate_every = 100
save_every = 1
quit_after_n_iters_without_improvement = 5000
per_process_gpu_memory_fraction = -1

#***************************************************************
# Miscellaneous
[Radam Optimizer]
name = radam
# TODO keep adjusting lr?
learning_rate = 2e-3
decay = .75
decay_steps = 5000
clip = 5
mu = .9
nu = .9
gamma = 0
chi = 0
epsilon = 1e-12

[Zipf]
n_zipfs = 3
name = zipf
filename = %(save_dir)s/%(name)s.txt
batch_size = 500
max_train_iters = 5000
print_every = 500

[Bucketer]
name = bucketer
filename = %(save_dir)s/%(name)s.txt
